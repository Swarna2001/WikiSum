{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Importing the NLTK library'''\n",
    "import nltk  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Importing the Wikipedia API for extracting Wikipedia articles'''\n",
    "import wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "word_tokenize - function to tokenize a given sentence into a list of words\n",
    "sent_tokenize - function to tokenize a given paragraph into a list of sentences\n",
    "stopwords - function to generate the list of stopwords pertaining to a given language\n",
    "PorterStemmer - algorithm to find out the root stem of a given word\n",
    "string - a class used to remove punctuations from the given text\n",
    "'''\n",
    "from nltk import word_tokenize\n",
    "from nltk import sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(wp):\n",
    "    '''\n",
    "    This function is used for preprocessing the data contained in the \n",
    "    supplied WikipediaPage object wp argument. It tokenises the given \n",
    "    data into a set of sentences first, followed by word level as well.\n",
    "    '''\n",
    "    \n",
    "    '''The content function is used to extract the content of a the Wikipedia article'''\n",
    "    webpageContent = wp.content\n",
    "    \n",
    "    '''Returns a list consisting of the sentences of the article'''\n",
    "    webPageSentences = sent_tokenize(webpageContent)\n",
    "    \n",
    "    '''Returns a list consisting of the sentences of the article without punctuations'''\n",
    "    webPageRemovedPunctuations = [\"\".join([char for char in s if char not in string.punctuation]) for s in webPageSentences]\n",
    "    \n",
    "    '''Returns a list consisting of the sentences of the article, where each sentence is now a list consisting \n",
    "        of the corresponding words that make it up'''\n",
    "    webPageWords = [word_tokenize(n) for n in webPageRemovedPunctuations]\n",
    "    \n",
    "    '''Stop words are removed'''\n",
    "    stop_words = stopwords.words(\"english\")\n",
    "    webPageFilteredWords = [[word for word in s if word not in stop_words] for s in webPageWords]\n",
    "    \n",
    "    '''Each word present is converted to its root stem using PorterStemmer Algorithm'''\n",
    "    porter = PorterStemmer()\n",
    "    webPageStemmed = [[porter.stem(word) for word in s] for s in webPageFilteredWords]\n",
    "    \n",
    "    return webPageStemmed\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Creating a string object storing the content of the Wikipedia article'''\n",
    "webPageContent = wikipedia.page(\"Natural_language_processing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Natural language processing (NLP) is a subfield of linguistics, computer science, and artificial intelligence concerned with the interactions between computers and human language, in particular how to program computers to process and analyze large amounts of natural language data.\n",
      "['Natural', 'language', 'processing', 'NLP', 'is', 'a', 'subfield', 'of', 'linguistics', 'computer', 'science', 'and', 'artificial', 'intelligence', 'concerned', 'with', 'the', 'interactions', 'between', 'computers', 'and', 'human', 'language', 'in', 'particular', 'how', 'to', 'program', 'computers', 'to', 'process', 'and', 'analyze', 'large', 'amounts', 'of', 'natural', 'language', 'data']\n",
      "['natur', 'languag', 'process', 'nlp', 'subfield', 'linguist', 'comput', 'scienc', 'artifici', 'intellig', 'concern', 'interact', 'comput', 'human', 'languag', 'particular', 'program', 'comput', 'process', 'analyz', 'larg', 'amount', 'natur', 'languag', 'data']\n"
     ]
    }
   ],
   "source": [
    "'''Processing the web page content'''\n",
    "processedText = preprocess(webPageContent)\n",
    "print(processedText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
