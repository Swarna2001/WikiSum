{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "'''\n",
    "word_tokenize - function to tokenize a given sentence into a list of words\n",
    "sent_tokenize - function to tokenize a given paragraph into a list of sentences\n",
    "stopwords - function to generate the list of stopwords pertaining to a given language\n",
    "PorterStemmer - algorithm to find out the root stem of a given word\n",
    "string - a class used to remove punctuations from the given text\n",
    "'''\n",
    "from nltk import word_tokenize\n",
    "from nltk import sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(givenFile):\n",
    "    '''\n",
    "    This function is used for preprocessing the data contained in the \n",
    "    supplied file object givenFile argument. It tokenises the given \n",
    "    data into a set of sentences first, followed by word level as well.\n",
    "    '''\n",
    "    \n",
    "    '''Returns a list consisting of the sentences of the article'''\n",
    "    webPageSentences = sent_tokenize(givenFile)\n",
    "    \n",
    "    '''Returns a list consisting of the sentences of the article without punctuations'''\n",
    "    webPageRemovedPunctuations = [\"\".join([char for char in s if char not in string.punctuation]) for s in webPageSentences]\n",
    "    \n",
    "    '''Returns a list consisting of the sentences of the article, where each sentence is now a list consisting \n",
    "        of the corresponding words that make it up'''\n",
    "    webPageWords = [word_tokenize(n) for n in webPageRemovedPunctuations]\n",
    "    \n",
    "    '''Stop words are removed'''\n",
    "    stop_words = stopwords.words(\"english\")\n",
    "    webPageFilteredWords = [[word for word in s if word not in stop_words] for s in webPageWords]\n",
    "    \n",
    "    '''Each word present is converted to its root stem using PorterStemmer Algorithm'''\n",
    "    porter = PorterStemmer()\n",
    "    webPageStemmed = [[porter.stem(word) for word in s] for s in webPageFilteredWords]\n",
    "    \n",
    "    return webPageStemmed\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "fileTitle = \"\"\n",
    "fileSummary = \"\"\n",
    "fileContent = \"\"\n",
    "\n",
    "check = 0\n",
    "with open('wikihowAll.csv', newline='', encoding='utf-8') as f:\n",
    "    reader = csv.reader(f)\n",
    "    for row in reader:\n",
    "        if check:\n",
    "            fileSummary = row[0]\n",
    "            fileTitle = row[1]\n",
    "            fileContent = row[2]\n",
    "            break\n",
    "        check += 1\n",
    "        \n",
    "print(\"ARTICLE TITLE : \\n {} \\n\\n ACTUAL CONTENT : \\n {} \\n\\n SUMMARY : \\n {} \\n\\n\".format(fileTitle, fileContent, fileSummary))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preProcessedContent = preprocess(fileContent)\n",
    "print(preProcessedContent)\n",
    "\n",
    "'''Average sentence length'''\n",
    "averageSentLength = 0\n",
    "\n",
    "'''Finding out the vocabulary of the document'''\n",
    "bagOfWords = []\n",
    "for i in preProcessedContent:\n",
    "    bagOfWords.extend(i)\n",
    "    averageSentLength += len(i)\n",
    "    \n",
    "bagOfWords = set(bagOfWords)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_vector():\n",
    "    ''' This function will return a tuple of 10 elements\n",
    "    corresponding to the value of the 10 features.'''\n",
    "    \n",
    "    '''f1 = Sentence position.We assume that the first sentences of a paragraph are the most important. \n",
    "    Therefore, we rank a paragraph sentence according to its position in the paragraph and we consider \n",
    "    maximum positions of 5.'''\n",
    "    positionOfSentence = preProcessedContent.index(sentence)\n",
    "    f1 = 0\n",
    "    if positionOfSentence in range(0, 5):\n",
    "        f1 = (5 - positionOfSentence) / 5\n",
    "    else:\n",
    "        f1 = 0\n",
    "        \n",
    "    ''' f4 = Sentence centrality (similarity with rest of document).Sentence centrality is the vocabulary overlap\n",
    "    between this sentence and other sentences in the document.'''\n",
    "    distinctWordsInSent = set(sentence)\n",
    "    f4 = len(distinctWordsInSent.intersection(bagOfWords)) / len(distinctWordsInSent.union(bagOfWords))\n",
    "    \n",
    "    '''f6 = Sentence Resemblance to the title.Sentence resemblance to the title is the vocabulary overlap\n",
    "    between this sentence and the document title.'''\n",
    "    distinctWordsInTitle = set(fileTitle)\n",
    "    f6 = len(distinctWordsInSent.intersection(distinctWordsInTitle)) / len(distinctWordsInSent.union(distinctWordsInTitle))\n",
    "    \n",
    "    \n",
    "    '''f7 = sentence relative length.This feature is employed to penalize sentences that are too short, since these\n",
    "    sentences are not expected to belong to the summary.'''\n",
    "    f7 = len(sentence) * averageSentLength\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_vector(preProcessedContent[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
